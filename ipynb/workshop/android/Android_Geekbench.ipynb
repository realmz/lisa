{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geekbench benchmark on Android\n",
    "\n",
    "Geekbench4 is an app offering several benchmarks to run on android smartphones. The one used in this notebook is the '**CPU**' benchmark, which runs several workloads that follow the lines of what is commonly run by smartphones (AES, JPEG codec, FFT, and so on). The benchmark runs all the tests in '**Single-Core**' mode as well as in '**Multi-Core**' in order to compare the single-thread and multi-thread performances of the device.\n",
    "\n",
    "**Do note that the benchmark will attempt to upload its results, which includes some hardware information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-17 15:42:31,638 INFO    : root         : Using LISA logging configuration:\n",
      "2018-08-17 15:42:31,640 INFO    : root         :   /home/steven/lisa/lisa-github/logging.conf\n"
     ]
    }
   ],
   "source": [
    "from conf import LisaLogging\n",
    "LisaLogging.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Support to access the remote target\n",
    "import devlib\n",
    "from env import TestEnv\n",
    "\n",
    "# Import support for Android devices\n",
    "from android import Screen, Workload\n",
    "\n",
    "# Support for trace events analysis\n",
    "from trace import Trace\n",
    "\n",
    "# Suport for FTrace events parsing and visualization\n",
    "import trappy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps us run our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    \n",
    "    # Configure governor\n",
    "    target.cpufreq.set_all_governors('schedutil')\n",
    "    \n",
    "    # Get workload\n",
    "    wload = Workload.getInstance(te, 'Geekbench')\n",
    "    \n",
    "    # Run Geekbench workload\n",
    "    wload.run(te.res_dir, test_name='CPU', collect='ftrace')\n",
    "        \n",
    "    # Dump platform descriptor\n",
    "    te.platform_dump(te.res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test environment setup\n",
    "For more details on this please check out **examples/utils/testenv_example.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**devlib** requires the ANDROID_HOME environment variable configured to point to your local installation of the Android SDK. If you have not this variable configured in the shell used to start the notebook server, you need to run a cell to define where your Android SDK is installed or specify the ANDROID_HOME in your target configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case more than one Android device are conencted to the host, you must specify the ID of the device you want to target in **my_target_conf**. Run **adb devices** on your host to get the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target configuration\n",
    "androidsdk_path=os.path.join(os.getcwd(), \"../../../../android-sdk\")\n",
    "my_conf = {\n",
    "\n",
    "#     # Target platform and board\n",
    "#     \"platform\"     : 'android',\n",
    "#     \"board\"        : 'pixel',\n",
    "    \n",
    "#     # Device\n",
    "#     \"device\"       : \"0123456789ABCDEF\",\n",
    "    \n",
    "#     # Android home\n",
    "#     \"ANDROID_HOME\" : \"/home/vagrant/lisa/tools/android-sdk-linux/\",\n",
    "    \"platform\"    : 'android',\n",
    "    \"board\"       : \"hikey960\",\n",
    "    \"device\" : \"0123456789ABCDEF\",\n",
    "    \n",
    "    \"ANDROID_HOME\" : androidsdk_path,\n",
    "    \"rtapp-calib\" : {\"0\": 302, \"1\": 302, \"2\": 304, \"3\": 304, \"4\": 136, \"5\": 137, \"6\": 136, \"7\": 136},\n",
    "\n",
    "     \"emeter\" : {\n",
    "        \"instrument\" : \"acme\",\n",
    "        \"conf\" : {\n",
    "            # Absolute path to the iio-capture binary on the host\n",
    "            'iio-capture' : '/usr/bin/iio-capture',\n",
    "            # Default host name of the BeagleBone Black\n",
    "#             'ip_address'     : '10.65.34.1',\n",
    "        },\n",
    "        \"channel_map\" : {\n",
    "            \"Device0\" : 0, # iio:device0\n",
    "            \"Device1\" : 1, # iio:device0\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Folder where all the results will be collected\n",
    "    \"results_dir\" : datetime.datetime.now()\\\n",
    "                    .strftime(\"Geekbench_example_\" + '%Y%m%d_%H%M%S'),\n",
    "\n",
    "    # Define devlib modules to load\n",
    "    \"modules\"     : [\n",
    "        'cpufreq'       # enable CPUFreq support\n",
    "    ],\n",
    "\n",
    "    # FTrace events to collect for all the tests configuration which have\n",
    "    # the \"ftrace\" flag enabled\n",
    "    \"ftrace\"  : {\n",
    "         \"events\" : [\n",
    "            \"sched_switch\",\n",
    "            \"sched_wakeup\",\n",
    "            \"sched_wakeup_new\",\n",
    "            \"sched_overutilized\",\n",
    "            \"sched_load_avg_cpu\",\n",
    "            \"sched_load_avg_task\",\n",
    "            \"cpu_capacity\",\n",
    "            \"cpu_frequency\",\n",
    "         ],\n",
    "         \"buffsize\" : 100 * 1024,\n",
    "    },\n",
    "\n",
    "    # Tools required by the experiments\n",
    "    \"tools\"   : [ 'trace-cmd', 'taskset'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a test environment using:\n",
    "te = TestEnv(my_conf, wipe=False)\n",
    "target = te.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workloads execution\n",
    "\n",
    "This is done using the **experiment** helper function defined above which is configured to run a **Geekbench - CPU** experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-86e54aff81de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize Workloads for this test environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-f60929ecb311>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Run Geekbench workload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mwload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CPU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ftrace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Dump platform descriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/steven/lisa/lisa-github/libs/utils/android/workloads/geekbench.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, out_dir, test_name, collect)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# read next logcat line (up to max 1024 chars)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Benchmark start trigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Workloads for this test environment\n",
    "results = experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Geekbench4 uses a baseline score of 4000, which is the benchmark score of an Intel Core i7-6600U. Higher scores are better, with double the score indicating double the performance. You can have a look at the results for several android phones here https://browser.primatelabs.com/android-benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geekbench(object):\n",
    "    \"\"\"\n",
    "    Geekbench json results parsing class\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as fd:\n",
    "            self.__json = json.loads(fd.read())\n",
    "        \n",
    "        self.benchmarks = {}\n",
    "        for section in self.__json[\"sections\"]:\n",
    "            self.benchmarks[section[\"name\"]] = section\n",
    "            for workload in section[\"workloads\"]:\n",
    "                self.benchmarks[section[\"name\"]][workload[\"name\"]] = workload     \n",
    "            \n",
    "    def name(self):\n",
    "        \"\"\"Get a human-readable name for the geekbench run\n",
    "        \"\"\"\n",
    "        gov = \"\"\n",
    "        build = \"\"\n",
    "        for metric in self.__json[\"metrics\"]:\n",
    "            if metric[\"name\"] == \"Governor\":\n",
    "                gov = metric[\"value\"]\n",
    "            elif metric[\"name\"] == \"Build\":\n",
    "                build = metric[\"value\"]\n",
    "\n",
    "        return \"[build]=\\\"{}\\\" [governor]=\\\"{}\\\"\".format(build, gov)\n",
    "    \n",
    "    def benchmarks_names(self):\n",
    "        \"\"\"Get a list of benchmarks (e.g. Single-Core, Multi-Core) found in the run results        \n",
    "        \"\"\"\n",
    "        return [section[\"name\"] for section in self.__json[\"sections\"]]\n",
    "    \n",
    "    def workloads_names(self):\n",
    "        \"\"\"Get a list of unique workloads (e.g. EAS, Dijkstra) found in the run results\n",
    "        \"\"\"\n",
    "        return [workload[\"name\"] for workload in self.benchmarks.values()[0][\"workloads\"]]\n",
    "    \n",
    "    def global_scores(self):\n",
    "        \"\"\"Get the overall scores of each benchmark\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for benchmark in self.benchmarks_names():\n",
    "            data[benchmark] = self.benchmarks[benchmark][\"score\"]\n",
    "        return data\n",
    "        \n",
    "    def detailed_scores(self):\n",
    "        \"\"\"Get the detailed workload scores of each benchmark\n",
    "        \"\"\"\n",
    "        benchmark_fields = [\"score\", \"runtime_mean\", \"rate_string\"]\n",
    "        benches = {}\n",
    "        benchmarks = self.benchmarks_names()\n",
    "        workloads = self.workloads_names() \n",
    "        \n",
    "        for benchmark in benchmarks:\n",
    "            data = {}\n",
    "            for workload in workloads:\n",
    "                data[workload] = {}\n",
    "                for field in benchmark_fields:\n",
    "                    data[workload][field] = self.benchmarks[benchmark][workload][field]        \n",
    "            benches[benchmark] = data\n",
    "            \n",
    "        return benches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bench_results(geekbench, detailed=False):\n",
    "    print \"===== Global results =====\"\n",
    "    \n",
    "    scores = geekbench.global_scores()\n",
    "    \n",
    "    # Build dataframe for display\n",
    "    row = []\n",
    "    for bench_type, score in scores.iteritems():\n",
    "        row.append(score)\n",
    "        \n",
    "    df = pd.DataFrame(data=row, index=scores.keys(), columns=[\"Global score\"])\n",
    "    display(df)\n",
    "    \n",
    "    if not detailed:\n",
    "        return\n",
    "    \n",
    "    print \"===== Detailed results =====\"\n",
    "    \n",
    "    scores = geekbench.detailed_scores()\n",
    "    \n",
    "    for benchmark, results in geekbench.detailed_scores().iteritems():\n",
    "        print \"----- {} benchmark -----\".format(benchmark)\n",
    "        # Build dataframe for display\n",
    "        data = []\n",
    "        idx = []\n",
    "        columns = results.values()[0].keys()\n",
    "        for workload, fields in results.iteritems():\n",
    "            data.append(tuple(fields.values()))\n",
    "            idx.append(workload)\n",
    "        display (pd.DataFrame(data=data, index=idx, columns=columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(te.res_dir):\n",
    "    if f.endswith(\".gb4\"):\n",
    "        geekbench = Geekbench(te.res_dir + \"/\" + f)\n",
    "        \n",
    "        print \"Analysing geekbench {}\".format(geekbench.name())\n",
    "        display_bench_results(geekbench, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing several runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be interesting to compare Geekbench results with different parameters (kernel, drivers) and even different devices to gauge the impact of these parameters. As Geekbench results can vary a bit from one run to another, having a set of repeated runs is preferable.\n",
    "\n",
    "The following section will grab the results of all the **Geekbench\\_exemple\\_\\*** results found in the LISA results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def fetch_results():\n",
    "    results_path = os.path.join(te.LISA_HOME, \"results\")\n",
    "    \n",
    "    results_dirs = [results_path + \"/\" + d for d in os.listdir(results_path) if d.startswith(\"Geekbench_example_\")]\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for d in results_dirs:\n",
    "        bench_file = glob.glob(\"{}/*.gb4\".format(d))[0]\n",
    "        res.append(Geekbench(bench_file))\n",
    "        \n",
    "    return res\n",
    "\n",
    "def compare_runs():\n",
    "    geekbenches = fetch_results()\n",
    "    \n",
    "    # Pick one run to build a baseline template\n",
    "    benchmarks = geekbenches[0].benchmarks_names()\n",
    "    workloads = geekbenches[0].workloads_names()\n",
    "    \n",
    "    stats  = [\"avg\", \"min\", \"max\"]\n",
    "    count = len(geekbenches)\n",
    "    \n",
    "    print \"Parsing {} runs\".format(count)\n",
    "\n",
    "    \n",
    "    # Initialize stats\n",
    "    results = {benchmark : \n",
    "                        {\"min\" : sys.maxint, \"max\" : 0, \"avg\" : 0} \n",
    "               for benchmark in benchmarks}\n",
    "    \n",
    "    # Get all the data\n",
    "    for benchmark in results.iterkeys():\n",
    "        for bench in geekbenches:\n",
    "            score = bench.global_scores()[benchmark]\n",
    "            \n",
    "            if score > results[benchmark][\"max\"]:\n",
    "                results[benchmark][\"max\"] = score\n",
    "                \n",
    "            if score < results[benchmark][\"min\"]:\n",
    "                results[benchmark][\"min\"] = score\n",
    "            \n",
    "            results[benchmark][\"avg\"] += score\n",
    "        \n",
    "        results[benchmark][\"avg\"] /= count\n",
    "        \n",
    "    # Convert data to Dataframe\n",
    "    data = []\n",
    "\n",
    "    for benchmark in results.iterkeys():\n",
    "        row = []\n",
    "        for stat in stats:\n",
    "            row.append(results[benchmark][stat])\n",
    "        data.append(tuple(row))\n",
    "       \n",
    "    df = pd.DataFrame(data, index=results.iterkeys(), columns=stats)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(compare_runs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
