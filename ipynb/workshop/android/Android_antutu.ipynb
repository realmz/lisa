{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antutu benchmark on Android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this experiment is to run benchmarks on a hikey960 board running Android with an EAS kernel and collect results. The analysis phase will consist in comparing EAS with other schedulers, that is comparing *sched* governor with:\n",
    "\n",
    "    - interactive\n",
    "    - performance\n",
    "    - schedutil\n",
    "    \n",
    "The benchmark we will be using is ***Antutu 3Dbench*** (http://www.antutu.com/en). You will need to **manually install** the app on the Android device in order to run this Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-17 15:32:27,513 INFO    : root         : Using LISA logging configuration:\n",
      "2018-08-17 15:32:27,515 INFO    : root         :   /home/steven/lisa/lisa-github/logging.conf\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from conf import LisaLogging\n",
    "LisaLogging.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import copy\n",
    "import os\n",
    "from time import sleep\n",
    "from subprocess import Popen\n",
    "import pandas as pd\n",
    "\n",
    "# Support to access the remote target\n",
    "import devlib\n",
    "from env import TestEnv\n",
    "\n",
    "# Support for trace events analysis\n",
    "from trace import Trace\n",
    "\n",
    "# Suport for FTrace events parsing and visualization\n",
    "import trappy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test environment setup\n",
    "\n",
    "For more details on this please check out **examples/utils/testenv_example.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case more than one Android device are conencted to the host, you must specify the ID of the device you want to target in `my_target_conf`. Run `adb devices` on your host to get the ID. Also, you have to specify the path to your android sdk in ANDROID_HOME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a target configuration\n",
    "androidsdk_path=os.path.join(os.getcwd(), \"../../../../android-sdk\")\n",
    "my_target_conf = {\n",
    "    \n",
    "#     # Target platform and board\n",
    "#     \"platform\"    : 'android',\n",
    "\n",
    "#     # Add target support\n",
    "#     \"board\" : 'pixel',\n",
    "    \n",
    "#     # Device ID\n",
    "#     \"device\" : \"HT6670300102\",\n",
    "    \n",
    "#     \"ANDROID_HOME\" : \"/home/vagrant/lisa/tools/android-sdk-linux/\",\n",
    "    \n",
    "    \"platform\"    : 'android',\n",
    "    \"board\"       : \"hikey960\",\n",
    "    \"device\" : \"0123456789ABCDEF\",\n",
    "    \n",
    "    \"ANDROID_HOME\" : androidsdk_path,\n",
    "    \"rtapp-calib\" : {\"0\": 302, \"1\": 302, \"2\": 304, \"3\": 304, \"4\": 136, \"5\": 137, \"6\": 136, \"7\": 136},\n",
    "\n",
    "     \"emeter\" : {\n",
    "        \"instrument\" : \"acme\",\n",
    "        \"conf\" : {\n",
    "            # Absolute path to the iio-capture binary on the host\n",
    "            'iio-capture' : '/usr/bin/iio-capture',\n",
    "            # Default host name of the BeagleBone Black\n",
    "            'ip_address'     : '10.65.34.1',\n",
    "        },\n",
    "        \"channel_map\" : {\n",
    "            \"Device0\" : 0, # iio:device0\n",
    "            \"Device1\" : 1, # iio:device0\n",
    "        }\n",
    "    },\n",
    "    # Define devlib modules to load\n",
    "    \"modules\"     : [\n",
    "        'cpufreq'       # enable CPUFreq support\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_tests_conf = {\n",
    "\n",
    "    # Folder where all the results will be collected\n",
    "    \"results_dir\" : \"Android_Antutu\",\n",
    "\n",
    "    # Platform configurations to test\n",
    "    \"confs\" : [\n",
    "        {\n",
    "            \"tag\"            : \"pcmark\",\n",
    "            \"flags\"          : \"ftrace\",           # Enable FTrace events\n",
    "            \"sched_features\" : \"ENERGY_AWARE\",     # enable EAS\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a test environment using:\n",
    "# the provided target configuration (my_target_conf)\n",
    "# the provided test configuration   (my_test_conf)\n",
    "te = TestEnv(target_conf=my_target_conf, test_conf=my_tests_conf)\n",
    "target = te.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of support functions will help us running the benchmark using different CPUFreq governors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_performance():\n",
    "    target.cpufreq.set_all_governors('performance')\n",
    "\n",
    "def set_powersave():\n",
    "    target.cpufreq.set_all_governors('powersave')\n",
    "\n",
    "def set_interactive():\n",
    "    target.cpufreq.set_all_governors('interactive')\n",
    "\n",
    "def set_sched():\n",
    "    target.cpufreq.set_all_governors('schedutil')\n",
    "\n",
    "def set_ondemand():\n",
    "    target.cpufreq.set_all_governors('ondemand')\n",
    "    \n",
    "    for cpu in target.list_online_cpus():\n",
    "        tunables = target.cpufreq.get_governor_tunables(cpu)\n",
    "        target.cpufreq.set_governor_tunables(\n",
    "            cpu,\n",
    "            'ondemand',\n",
    "            **{'sampling_rate' : tunables['sampling_rate_min']}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUFreq configurations to test\n",
    "confs = {\n",
    "    'performance' : {\n",
    "        'label' : 'prf',\n",
    "        'set' :  set_performance,\n",
    "    },\n",
    "    #'powersave' : {\n",
    "    #    'label' : 'pws',\n",
    "    #    'set' :  set_powersave,\n",
    "    #},\n",
    "    'sched' : {\n",
    "       'label' : 'sch',\n",
    "       'set' :  set_sched,\n",
    "    },\n",
    "    #'ondemand' : {\n",
    "    #    'label' : 'odm',\n",
    "    #    'set' :  set_ondemand,\n",
    "    #}\n",
    "}\n",
    "\n",
    "# The set of results for each comparison test\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if PCMark si available on the device\n",
    "\n",
    "def check_packages(pkgname):\n",
    "    try:\n",
    "        output = target.execute('pm list packages -f | grep -i {}'.format(pkgname))\n",
    "    except Exception:\n",
    "        raise RuntimeError('Package: [{}] not availabe on target'.format(pkgname))\n",
    "\n",
    "# Check for specified PKG name being available on target\n",
    "check_packages('com.antutu.ABenchMark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from android import Screen\n",
    "\n",
    "target.adb_root(force=True)\n",
    "target.execute('am force-stop {}'.format(\"com.antutu.ABenchMark\"))\n",
    "target.execute('input keyevent 4')\n",
    "Screen.set_orientation(target, auto=False, portrait=True)\n",
    "Screen.set_brightness(target, auto=False, percent=0)\n",
    "target.cpufreq.set_all_governors('schedutil')\n",
    "\n",
    "# Start PCMark on the target device\n",
    "#target.execute('monkey --pct-syskeys 0 -p com.antutu.ABenchMark -c android.intent.category.LAUNCHER 1')\n",
    "# Wait few seconds to make sure the app is loaded\n",
    "# Screen.set_orientation(target, auto=False, portrait=True)\n",
    "#sleep(10)\n",
    "#target.execute('input tap 961 202')\n",
    "# Wait few seconds to make sure the app is loaded\n",
    "# sleep(5)\n",
    "    \n",
    "\n",
    "# Run performance workload (assume screen is vertical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that helps run a Antutu experiment\n",
    "from android import Screen\n",
    "\n",
    "def benchmark_run(exp_dir):\n",
    "    # Unlock device screen (assume no password required)\n",
    "    target.execute('am force-stop {}'.format(\"com.antutu.ABenchMark\"))\n",
    "    target.execute('input keyevent 82')\n",
    "    Screen.set_brightness(target, auto=False, percent=0)\n",
    "\n",
    "    # Start PCMark on the target device\n",
    "    target.execute('monkey --pct-syskeys 0 -p com.antutu.ABenchMark  1')\n",
    "#     Screen.set_orientation(target, auto=False, portrait=True)\n",
    "\n",
    "    # Wait few seconds to make sure the app is loaded\n",
    "    sleep(5)\n",
    "    \n",
    "    # Flush entire log\n",
    "    target.clear_logcat()\n",
    "    target.execute('input tap 860 1000')\n",
    "    sleep(1)\n",
    "    Screen.set_orientation(target, auto=False, portrait=True)\n",
    "    # Run performance workload (assume screen is vertical)\n",
    "    target.execute('input tap 960 200')\n",
    "    # Wait for completion (10 minutes in total) and collect log\n",
    "#     log_file = os.path.join(exp_dir, 'log.txt')\n",
    "    # Wait 5 minutes\n",
    "    sleep(400)\n",
    "    # Start collecting the log\n",
    "#     with open(log_file, 'w') as log:\n",
    "#         logcat = Popen(['adb logcat', 'com.antutu.ABenchMark.VirtualMachineState:*', '*:S'],\n",
    "#                        stdout=log,\n",
    "#                        shell=True)\n",
    "#         # Wait additional two minutes for benchmark to complete\n",
    "#         sleep(300)\n",
    "\n",
    "#         # Terminate logcat\n",
    "#         logcat.kill()\n",
    "\n",
    "#     # Get scores from logcat\n",
    "#     score_file = os.path.join(exp_dir, 'score.txt')\n",
    "#     os.popen('grep -o \".*_SCORE .*\" {} | sed \"s/ = / /g\" | sort -u > {}'.format(log_file, score_file))\n",
    "    \n",
    "    # Close application\n",
    "    target.execute('am force-stop com.antutu.ABenchMark')\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that helps run PCMark for different governors\n",
    "\n",
    "def experiment(governor, exp_dir):\n",
    "    os.system('mkdir -p {}'.format(exp_dir));\n",
    "\n",
    "    logging.info('------------------------')\n",
    "    logging.info('Run workload using %s governor', governor)\n",
    "    confs[governor]['set']()\n",
    "\n",
    "    ### Run the benchmark ###\n",
    "    benchmark_run(exp_dir)\n",
    "    \n",
    "    # return all the experiment data\n",
    "    return {\n",
    "        'dir'        : exp_dir,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Antutu and collect scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-17 15:37:04,155 INFO    : root         : ------------------------\n",
      "2018-08-17 15:37:04,157 INFO    : root         : Run workload using performance governor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ffbeb4bfb65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgovernor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgovernor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgovernor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mgovernor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-749c7ce6bc60>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(governor, exp_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m### Run the benchmark ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbenchmark_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# return all the experiment data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-7cdd112aaa9c>\u001b[0m in \u001b[0;36mbenchmark_run\u001b[0;34m(exp_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     log_file = os.path.join(exp_dir, 'log.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Wait 5 minutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Start collecting the log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     with open(log_file, 'w') as log:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the benchmark in all the configured governors\n",
    "for governor in confs:\n",
    "    test_dir = os.path.join(te.res_dir, governor)\n",
    "    res = experiment(governor, test_dir)\n",
    "    print governor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the benchmark for the specified governors we can show and plot the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "data = {}\n",
    "data['sched'] = {}\n",
    "data['sched']['total'] = 102427\n",
    "data['sched']['3D'] = 24634\n",
    "data['sched']['UX'] = 37816\n",
    "data['sched']['CPU'] = 30584\n",
    "data['sched']['RAM'] = 9393\n",
    "\n",
    "data['performance'] = {}\n",
    "data['performance']['total'] = 98792\n",
    "data['performance']['3D'] = 22808\n",
    "data['performance']['UX'] = 38565\n",
    "data['performance']['CPU'] = 28936\n",
    "data['performance']['RAM'] = 8483\n",
    "\n",
    "data['interactive'] = {}\n",
    "data['interactive']['total'] = 96260\n",
    "data['interactive']['3D'] = 21721\n",
    "data['interactive']['UX'] = 35607\n",
    "data['interactive']['CPU'] = 30439\n",
    "data['interactive']['RAM'] = 8493\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', rot=45, figsize=(16,8),\n",
    "        title='ANtutu scores Performance vs SchedFreq governors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
